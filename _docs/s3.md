---
layout: default
---

## Amazon Simple Storage Service (S3)

Amazon S3 is storage for the internet and can be used to store and retrieve any amount of data at any time, from anywhere. It can be used to store and protect any amount of data for various use cases such as data lakes, websites, mobile apps, backups, archives, IoT devices and big data analytics. 

S3 stores data as *objects* within *buckets*. An object is a file and any optional metadata that describes the file, and is uploaded to a bucket, which is a container for objects.
  * Buckets have a naming convention: <https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html#bucketnamingrules>
  * Buckets are created at the region level, and cannot be changed after created
  * Objects have a key, which is the full path of its location in the bucket (e.g. s3://my-bucket/folder/another_folder/file.txt)
  * Maximum object size is 5TB; if uploading more than 5GB, 'multi-part uploading' must be used

*Versioning* - Versioning allows objects within a bucket to have versions, which can protect against unintended deletes because previous versions can be restored. Any files not versioned before enabling versioning will have a version 'null'. Suspending versioning does not delete previous versions.

Deleting a versioned object will create a delete marker, which makes it appear as though the object has been deleted. To restore the object, delete the delete marker.

**S3 Storage Classes**

*S3 Standard General Purpose* - Default standard storage class with high availability across mutiple AZs. Can sustain 2 concurrent facility failures.

*S3 Standard Infrequent Access (IA)* - For data that is less frequently accessed, but requires rapid access when needed, such as disaster recovery and backups.
  * High availability across multiple AZs
  * Lower cost compared to General Purpose
  * Sustain 2 concurrent facility failures

*S3 One Zone IA* - Same as Standard IA, but data is stored in only one AZ which lowers the cost. However, data is not resilient to physical loss of the AZ. Recommended for data that can be re-created in case of an AZ failure, or for Cross Region Replication.

*S3 Intelligent Tiering* - Optimizes storage costs by automatically moving data to the most cost-effective tier based on access patterns. Best for data that has unknown or changing access patterns. There is a small monthly monitoring and auto-tiering fee.

*S3 Glacier* - Low cost storage used for archived data which is accessed very infrequently. Data is not retrievable instantly, and may take minutes to hours to retrieve.
  * Each item in Glacier is called an *Archive* and can be up to 40TB in size
  * Instead of buckets, Glacier has *Vaults*
  * Minimum storage duration of 90 days
  * There are three retrieval options:
    - Expedited - Takes 1 to 5 minutes to retreive, but cost is high
    - Standard - Takes 3 to 5 hours to retrieve
    - Bulk - Takes 5 to 12 hours to retrieve

*S3 Glacier Deep Archive* - Lowest storage cost option available where archived data is rarely accessed. Glacier Deep Archive has a minimum storage duration period of 180 days and a default retrieval time of 12 hours. 
  * Deep Archive has two retrieval options
    - Standard - Object restored within 12 hours
    - Bulk - Object restored within 48 hours

**S3 Lifecycle Rules**

Lifecycle rules can be set to transition objects between different storage classes based on access patterns. Rules can be created for a certain prefix ( e.g. *s3://somebucket/logs/\** ) or for certain object tags (e.g. *Department: Finance* )

*Transition actions* - Defines when objects are transitioned to another storage class; e.g. Move objects to Standard IA 60 days after creation, then to Glacier after 6 months.

*Expiration actions* - Configure objects to expire (delete) after a certain amount of time. Can be used to delete old versions if versioning is available, and to delete incomplete multi-part uploads.

**S3 Object Lock**

Can be used to adopt a WORM (Write Once Read Many) model, which blocks an object version deletion for a specified amount of time. This can be helpful for compliance and data retention.

**S3 Performance**

*Baseline performance* - S3 automatically scales  to high request latencies; 100-200 ms. An application can achieve at least 3500 PUT/COPY/POST/DELETE and 5500 GET/HEAD requests per second per prefix in a bucket, and there are no limits to the number of prefixes in a bucket. 

*KMS Limitation* - SSE-KMS may impact performance due to the *GenerateDataKey* KMS API call during uploads, and the *Decrypt* KMS API call during downloads.

*Multi-Part Uploads* - For larger file uploads, breaks up the file into smaller parts and uploads in parallel. Recomended for >100MB, required for >5GB

*S3 Transfer Acceleration* - For uploads only, can increase upload speeds by using an AWS edge location closer to the user. The edge location then transfers the file through the fast AWS private network to S3. Is compatible with multi-part upload.

*S3 Byte-Range Fetches* - Can be used to speed up dowloads by parallelizing GETs, requesting specific byte ranges. Can also be used to retrieve only parts of a file; e.g. the head of a file.

*S3 Select & Glacier Select* - Enables the ability to retrieve only a subset of data from an object in S3 by using simple SQL expressions, increasing performance.

**S3 Encryption**

Encryption of objects in a bucket can be enabled during upload of an object or can be set by default at the bucket level. There are 4 methods of encrypting objects in S3

*SSE-S3* - Encrypts each object with a unique key handled & managed by Amazon S3, and also encrypts the key itself with a master key that is regularly rotated. Encryption is performed using AES-256. 
  * Objects are encrypted server side
  * Encryption is performed using AES-256
  * To enable SSE-S3, an HTTP header must be set: ```"x-amz-server-side-encryption": "AES256"```

*SSE-KMS* - Encrypts objects using Amazons KMS service. S3 uses KMS Customer Master Keys (CMKs) to encrypt only the objects; any object metadata is not encrypted.
  * Objects are encrypted server side
  * KMS provides advantages of creating policies and audit trails
  * The AWS KMS CMK must be in the same Region as the bucket
  * AWS managed CMK or customer managed CMKs can be used
  * To enable SSE-KMS, must set HTTP header ```"x-amz-server-side-encryption": "aws:kms"```

*SSE-C* - Encrypts objects server-side using a client provided encryption key, using AES-256. After encryption, the key is discarded. Instead of storing the encryption key, S3 stores a salted HMAC value of the key to validate future requests. This HMAC value cannot be used to derive the original key or to decrypt objects.
  * Objects are encrypted server side
  * Cannot be used from the Web UI
  * HTTPS must be used (the key is sent in the request)
  * Each object can be encrypted using different keys
  * Key management is the responsibility of the client

*Client side encryption* - Encrypting data before sending it to S3. 
  * Clients must encrypt/decrypt data themselves
  * Can use KMS CMK for client-side encryption or a master key stored within an application

**S3 Policies**

S3 Policies control access to S3 buckets and objects. Theres two main types of policies; user policies and bucket policies.

*User Policies* - User policies are IAM based policies that can define which API calls should be allowed for a specific user from the IAM console.
 * An IAM principle can access an S3 object if the user IAM permissions allow it or the resource policy allows it, and there no explicity DENY

*Bucket Policies* - A resource based policy which allows to control access and other functions to an S3 bucket from the S3 console. Bucket policies can be used to grant public access, force objects to be encrypted at upload or grant access to another account.

Policies contain the following:
  * Resources - The AWS resource, defined by its Amazon Resource Name (ARN). These are buckets, objects, access points and jobs.
  * Actions - A set of API operations; e.g. s3:ListBucket, s3:GetObject
  * Effect - What effect will be when a user requests the specific action - set to either Allow or Deny. Default is Deny
  * Principal - The user, account, service or other entity that is the recipient of this permission
  * Condition - Conditions for when a policy is in effect

Example policy:
Notice the first resource is tailed by a ```/*```. This is needed to apply to objects within the bucket

  ```
  {
    "Version": "2012-10-17",
    "Id": "ExamplePolicy01",
    "Statement": [
        {
            "Sid": "ExampleStatement01",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::123456789012:user/Dave"
            },
            "Action": [
                "s3:GetObject",
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::awsexamplebucket1/*",
                "arn:aws:s3:::awsexamplebucket1"
            ]
        }
    ]
  }
  ```


**CORS (Cross-Origin Resource Sharing)**

Defines a way for client web apps loading in one domain to interact with resources in a different domain; allows cross-origin access to S3 resources.
  * An *origin* is a scheme (protocol like HTTP), host (domain) and port.
  * Same origin: http://example.com/app1 and http://example.com/app2
  * Different origins: http://www.example.com and http://other.example.com
  * Requests wont be fulfilled unless the other origin allows for the requests using CORS headers (e.g. Access-Control-Allow-Origin)

In order for the main webpage to be able to fetch someting (an html file for example) from another S3 bucket (which has a different URL, or Origin), the second bucket needs to have a CORS setting which sets the HTTP response header of Access-Control-Allow-Origin to the URL of the main webpage. CORS policies are written in JSON in the AWS console, under Permissions.

Example CORS policy:
Note the AllowedOrigins field is set to the URL of the main webpage, without an ending slash

    ```
    [
        {
            "AllowedHeaders": ["Authorization"],
            "AllowedMethods": ["GET"],
            "AllowedOrigins": ["http://coffeintheclouds-first-bucket.s3-website-us-east-1.amazonaws.com"],
            "ExposeHeaders": [],
            "MaxAgeSeconds": 3000
        }
    ]
    ```

**S3 Data Consistency Model**

S3 provides strong read-after-write consistency for PUTs and DELETEs ob objects in S3 buckets. Read operations on S3 Select, S3 Access Control Lists, S3 Object Tags and object metadata are also strongly consistent. Any read (GET or LIST) initiated following a PUT response will return the new data.

Updates to a single key are atomic; if PUT is executed on the same object by two concurrent threads, a read will return either the old or new data, but never partial or corrupt data.

Scenarios:

 * If a write by one client does not complete before the read of a second client, the second client might get either the old or the new data.
 * If one write starts before another write has finished (they are concurrent), S3 internally uses last-writer-wins semantics to determine which write takes precedence. However, factors such as network latency must be considered; the first write might be farther away than the second write, thus having longer latency.

Buckets have an eventual consistency model:
  * If a bucket is deleted then a LIST of all buckets occurs immediately afterwards, the deleted bucket might still appear.
  * If versioning is enabled for the first time on a bucket, it might take a short amount of time for the change to fully propogate.


**MFA Delete**

MFA-Delete adds an extra layer of security to an S3 bucket by requiring MFA when permanently deleting an object version or suspending/disabling versioning for a bucket.

Only the root account can enable or disable MFA Delete and the process must be performed using the AWS CLI.

**S3 Access Logs**

Server Access Logging for an S3 bucket can be enabled to log detailed records of requests made to the bucket. This can be useful for security and access audits, or to help understand more about a customer base or an S3 bill.

When enabled, you specify an S3 bucket to PUT log files into. **Do Not** set the current bucket as the bucket to store the log files; this will cause a loop where a request causes a logging event, which in turn causes another request, and so on.

When enabling S3 Logging, Amazon will automatically give Log Delivery group write permissions on the bucket where the access logs will be saved.

**S3 Event Notifications**

S3 buckets can be configured to send events to Lambda funcitons, SQS Queues or SNS whenever an object is created, deleted, etc. Notifications can deliver events in a few seconds, but can sometimes take longer. 

If two writes happen to a single non-versioned object at the same time, only a single event may be sent. To keep this from happening, enable versioning on the bucket.

**S3 Replication**

S3 replication can be enabled between buckets to enable automatic, asynchronous copying of objects. Buckets can be in the same region, different regions, or in different AWS accounts. Objects can be replicated to a single desitation bucket or to multiple replication buckets. An IAM Role is required to allow replication to happen; this role can be generated automatically by AWS when creating a replication configuration.

Use cases:
  * Replicate objects while retaining metadata - Important if its required that the replicated object is identical to the source object
  * Replicate objects into different storage classes - Use replication to put objects into S3 Glacier, Glacier Deep Archive, or another storage class in the destination buckets.
  * Maintain object copies under different ownership
  * Keep objects stored over multiple AWS Regions - Useful for meeting certain compliance requirements
  * Replicate objects witin 15 minutes - Can use Replication Time Control (S3 RTC) to replicate data in a predictible time frame
  * Live replication between prod and test environments

Things to know about replication:
  * After activation replication, object currently in the source bucket are not copied to the destination bucket.
  * Delete markers are not replicated by default
  * There is no chaining of replication - If a source bucket is replicating to a destination bucket (bucket 2), which is replicating to another destination bucket (bucket 3), bucket 2 will not replicate bucket 1 to bucket 3.

**Presigned URLs**

Presigned URLs can be used to give object acces to outside users when a bucket is not public. The presigned URL is generated using the AWS account's security credentials, and are only valid for a specified amount of time. For example, a presigned URL can be generated to allow an outside user to download a video from a private S3 bucket.

Anyone with valid security credentials can generate a presigned URL, but the user must have permisions to perform the specified operation on the S3 bucket (e.g. GET, PUT, etc.)

The presigned URL can be generated using the AWS CLI command ```aws s3 presign```

```aws s3 presign s3://mybucket/myobject --region <region>```

An expiration time can be set like so (expires in seconds):

```aws s3 presign s3://mybucket/myobject --expires-in 300 --region <region>```

**AWS Athena**

Athena is a serverless query service which allows to analyze data directly in S3 using SQL. Data in S3 buckets can be CSV, JSON, or columnar data formats like Apache Parquet or Apache ORC. It also has JDBC and ODBC drivers.

Athena can be used for Business intelligence, analytics or reporting, to query VPC flow logs, ELB logs, CloudTrail trails, etc. 

Anytime data is to be analyzed directly in S3, Athena should be used.